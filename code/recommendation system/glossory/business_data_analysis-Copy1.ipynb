{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154606, 11)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load necessary packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### load business data and review data\n",
    "trainSize=10000\n",
    "business_train = pd.read_json('..//data//business_train.json',orient = 'records',lines = True)\n",
    "review_train=pd.read_json('..//data//review_train.json',orient='records',lines=True,chunksize=trainSize)\n",
    "topic=pd.read_csv(\"../data/yaoshen.csv\")\n",
    "business_train=business_train.set_index('business_id').join(topic.set_index('business_id')[['fastfood','bars','japan','asian']],how='right')\n",
    "business_train=business_train[business_train.index.isin(id)]\n",
    "business_train['business_id'] = business_train.index\n",
    "business_train=business_train.reset_index(drop=True)\n",
    "\n",
    "### train xgboost models and do kruskal tests on each category of data\n",
    "\n",
    "# filter the data\n",
    "for i in range(4):\n",
    "    if i==0:   \n",
    "        label=\"fastfood\"\n",
    "        business_train_subset=business_train[business_train['fastfood']==1]\n",
    "    elif i==1:\n",
    "        label=\"bars\"\n",
    "        business_train_subset=business_train[business_train['bars']==1]\n",
    "    elif i==2:\n",
    "        label=\"japan\"\n",
    "        business_train_subset=business_train[business_train['japan']==1]\n",
    "    else:\n",
    "        label=\"asian\"\n",
    "        business_train_subset=business_train[business_train['asian']==1]\n",
    "     \n",
    "    # extract the nested json attributes\n",
    "    def nestedValue(x:str):\n",
    "        \"\"\"judge if a string contains nested json information\"\"\"\n",
    "        return re.match(u'{.*}',x)\n",
    "    colnames=[]\n",
    "    for attribute in business_train_subset.attributes:\n",
    "        if attribute==None:\n",
    "            continue\n",
    "        for key,value in attribute.items():\n",
    "            if not nestedValue(value):\n",
    "                colnames.append(key)\n",
    "            else:\n",
    "                for nestKey,nestValue in ast.literal_eval(value).items():\n",
    "                    colnames.append(nestKey)\n",
    "    colnames=set(colnames)\n",
    "    res=defaultdict(list)\n",
    "    count=0\n",
    "    for attribute in business_train_subset.attributes:\n",
    "        for i in colnames:\n",
    "            res[i].append(None)\n",
    "        if attribute==None:\n",
    "            count+=1\n",
    "            continue\n",
    "        for key,value in attribute.items():\n",
    "            if not nestedValue(value):\n",
    "                res[key][count]=value\n",
    "            else:\n",
    "                for nestKey,nestValue in ast.literal_eval(value).items():\n",
    "                    res[nestKey][count]=nestValue\n",
    "        count+=1 \n",
    "    attributeTrain=pd.DataFrame(res)\n",
    "    attributeTrain.shape\n",
    "    attributeTrain['business_id']=business_train_subset.business_id.tolist()\n",
    "\n",
    "    # join business data and review rating on business_id\n",
    "    shop_stars=pd.read_json(\"average_star_by_shops.json\",orient=\"records\")\n",
    "    attributeTrain=attributeTrain.join(shop_stars.set_index('business_id'),on=\"business_id\",how='left')\n",
    "\n",
    "    # data cleaning convert string to category \n",
    "    attributeTrainReduced=attributeTrain\n",
    "    attributeTrainReduced.fillna(value=pd.np.nan, inplace=True) # fill np.nan to represent missing values\n",
    "    for i in range(attributeTrainReduced.shape[0]):\n",
    "        for j in range(attributeTrainReduced.shape[1]):\n",
    "            if attributeTrainReduced.iloc[i,j]==\"None\" or attributeTrainReduced.iloc[i,j]==\"none\": # convert 'none' to np.nan\n",
    "                attributeTrainReduced.iloc[i,j]=np.nan\n",
    "    for i in range(attributeTrainReduced.shape[1]):\n",
    "        for j in range(attributeTrainReduced.shape[0]):\n",
    "            if pd.isna(attributeTrainReduced.iloc[j,i]):\n",
    "                continue\n",
    "            if attributeTrainReduced.iloc[j,i] not in ['\\'','u']:\n",
    "                continue\n",
    "            try:\n",
    "                attributeTrainReduced.iloc[j,i]=re.sub('u\\'(.*)\\'','\\\\1',attributeTrainReduced.iloc[j,i]) # convert u'sth' to 'sth'\n",
    "                attributeTrainReduced.iloc[j,i]=re.sub('\\'(.*)\\'','\\\\1',attributeTrainReduced.iloc[j,i])\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    # give zhengdong zhou for further processing\n",
    "    attributeTrainReduced.to_csv(label+\".csv\")\n",
    "    attributeTrainReduced=attributeTrainReduced.drop(\"business_id\",axis=1)\n",
    "\n",
    "    # xgboost for feature selection\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(attributeTrainReduced.iloc[:,:-1]),attributeTrainReduced.iloc[:,-1], random_state=0) # split the data into training set and test set\n",
    "    colLabels=pd.get_dummies(attributeTrainReduced.iloc[:,:-1]).columns # all column names of the dummy dataset    \n",
    "    seed = 7 # set seed to repeat my work\n",
    "    model = xgboost.XGBRegressor() # set a xgboost model \n",
    "    model.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_test, y_test)],eval_metric='rmse',verbose=False) # fit the model using the training data\n",
    "    y_pred = model.predict(X_test) # make predictions for test data\n",
    "    nameImportancePair=[(x,y) for x,y in zip(colLabels,model.feature_importances_)] \n",
    "    nameImportancePair=sorted(nameImportancePair,key=lambda x:x[1],reverse=True) # sort the column names by feature importance in descending order    \n",
    "    def get_important_feature(x,y): \n",
    "        \"\"\"\n",
    "        x is a dummy feature, y is the colnames list\n",
    "        return the index of columns with that feature\n",
    "        \"\"\"\n",
    "        originFeature=re.sub(\"()_.*\",\"\\\\1\",x)\n",
    "        index=[True if re.match(originFeature,x) else False for x in y]\n",
    "        return index\n",
    "\n",
    "\n",
    "    # Do kruskal test on feature\n",
    "    from  scipy.stats import kruskal\n",
    "    def difference_test(data):\n",
    "        \"\"\"\n",
    "        the input data is like this format: the last column is the average stars,\n",
    "        other columns are attribute levels, this function will test if the attribute\n",
    "        levels are significant factors. When we have two levels, we t test, else use \n",
    "        anova\n",
    "        \"\"\"\n",
    "        # kruskal test\n",
    "        alpha=0.1 # if p-value is smaller than alpha, we reject the null hypothesis\n",
    "        res=[]\n",
    "        for i in range(0,data.shape[1]-1):\n",
    "            res.append(data[data.iloc[:,i]==1].iloc[:,-1].values)\n",
    "        if len(res)<2:\n",
    "            return False\n",
    "        result,pValue=kruskal(*res) # this part is tricky, it's a pointer similar like c++\n",
    "        if pValue<alpha:\n",
    "            largestStar=list(map(np.mean,res))\n",
    "            starIndex=sorted(range(len(largestStar)), key=lambda k: largestStar[k])\n",
    "            level=[data.columns[x] for x in starIndex[::-1]]\n",
    "            return (True,level)\n",
    "        else:\n",
    "            return False\n",
    "    dummyBusiness=pd.get_dummies(attributeTrainReduced.iloc[:,:-1])\n",
    "    maxFeature=40\n",
    "    testedFeature=[]\n",
    "    for i in range(min(maxFeature,len(nameImportancePair))):\n",
    "        if re.sub(\"()_.*\",\"\\\\1\",nameImportancePair[i][0]) in testedFeature:\n",
    "            continue\n",
    "        testedFeature.append(re.sub(\"()_.*\",\"\\\\1\",nameImportancePair[i][0]))\n",
    "        dummyBusinessTemp=dummyBusiness.iloc[:,get_important_feature(nameImportancePair[i][0],colLabels)]\n",
    "        dummyBusinessTemp['stars']=attributeTrainReduced.iloc[:,-1]\n",
    "        print(difference_test(dummyBusinessTemp))\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
