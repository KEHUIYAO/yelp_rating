{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data1 = pd.read_csv('./01.csv',)\n",
    "data1 = data1.loc[:,data1.columns[2:]]\n",
    "\n",
    "i = 2\n",
    "while i <= 5:\n",
    "    data = pd.read_csv('./0%d.csv'%i)\n",
    "    data = data.loc[:,data.columns[2:]]\n",
    "    data1 = pd.concat([data1,data])\n",
    "    i = i +1\n",
    "\n",
    "del data\n",
    "\n",
    "ids = data1.business_id.unique()\n",
    "np.savetxt('id.txt',ids,delimiter='\\n')\n",
    "\n",
    "stars = []\n",
    "reviews = []\n",
    "for i in tqdm(range(len(ids))):\n",
    "    j = ids[i]\n",
    "    tmp = data1[data1.business_id == j]\n",
    "    star = [x for x in tmp.stars.values]\n",
    "    review = ''\n",
    "    for y in tmp.text.values:\n",
    "        review = review + str(y) + ' '\n",
    "    stars.append(star)\n",
    "    reviews.append(review)\n",
    "\n",
    "final = pd.DataFrame({'business_id':ids,'stars':stars,'reviews':reviews})\n",
    "del data1\n",
    "\n",
    "del ids,stars,reviews\n",
    "\n",
    "final.to_csv('final.csv')\n",
    "final = pd.read_csv('final.csv')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(final.reviews.values, min_count=10, threshold=10)\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "bireviews = bigram[final.reviews.values]\n",
    "final.reviews = bireviews\n",
    "\n",
    "final.to_csv('bifinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "bifinal = pd.read_csv('bifinal.csv',index_col=0)\n",
    "reviews = bifinal.reviews\n",
    "X = tfidf.fit_transform(reviews)\n",
    "names = tfidf.get_feature_names()\n",
    "with open('features.txt','w') as f:\n",
    "    for x in names:\n",
    "        f.write(x + '\\n')\n",
    "def save_sparse_csr(filename, array):\n",
    "    \n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "save_sparse_csr(filename='tfidf',array=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "bifinal = pd.read_csv('bifinal.csv',index_col=0)\n",
    "reviews = bifinal.reviews\n",
    "X = tfidf.fit_transform(reviews)\n",
    "names = tfidf.get_feature_names()\n",
    "with open('features.txt','w') as f:\n",
    "    for x in names:\n",
    "        f.write(x + '\\n')\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    \n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "save_sparse_csr(filename='tfidf',array=X)\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "with open('features.txt') as f:\n",
    "    name = f.readlines()\n",
    "names = [re.sub('\\n','',x) for x in name]\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    \n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "X = load_sparse_csr('tfidf')\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "for i in range(5000):\n",
    "    feature = []\n",
    "    tmp = X.getrow(i).toarray()[0]\n",
    "    for x in range(10):\n",
    "        large = tmp.argmax()\n",
    "        tmp[large] = 0\n",
    "        feature.append(names[large])\n",
    "    features.append(feature)\n",
    "with open('1.txt','w') as f:\n",
    "    for line in features:\n",
    "        for i in range(10):\n",
    "            if i != 9:\n",
    "                f.write(line[i] + ',')\n",
    "            else:\n",
    "                f.write(line[i] + '\\n')\n",
    "\n",
    "features = []\n",
    "for i in range(X.shape[0]):\n",
    "    feature = []\n",
    "    tmp = X.getrow(i).toarray()\n",
    "    tuples = sorted(enumerate(tmp), key=lambda x: x[1])[:5]\n",
    "    for t in tuples:\n",
    "        feature.append(names[t[0]])\n",
    "    features.append(feature)\n",
    "\n",
    "with open('features.txt','w') as f:\n",
    "    for x in names:\n",
    "        f.write(x + '\\n')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    \n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    \n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "save_sparse_csr(filename='tfidf',array=X)\n",
    "\n",
    "X = load_sparse_csr('tfidf')\n",
    "\n",
    "features = []\n",
    "for i in range(X.shape[0]):\n",
    "    feature = []\n",
    "    tmp = X.getrow(i).toarray()[0]\n",
    "    tuples = sorted(enumerate(tmp), reverse=True,key=lambda x: x[1])[:5]\n",
    "    for t in tuples:\n",
    "        feature.append(names[t[0]])\n",
    "    features.append(feature)\n",
    "bifinal = bifinal.drop('reviews',axis=1)\n",
    "bifinal['top'] = features\n",
    "bifinal.to_csv('final.csv')\n",
    "\n",
    "features = []\n",
    "for i in tqdm(range(X.shape[0])):\n",
    "    feature = []\n",
    "    tmp = X.getrow(i).toarray()[0]\n",
    "    tuples = sorted(enumerate(tmp), reverse=True,key=lambda x: x[1])[:5]\n",
    "    for t in tuples:\n",
    "        feature.append(names[t[0]])\n",
    "    features.append(feature)\n",
    "\n",
    "with open('features.txt') as f:\n",
    "    name = f.readlines()\n",
    "\n",
    "names = [re.sub('\\n','',x) for x in name]\n",
    "\n",
    "restaurants = pd.read_csv('../../yaoshen.csv',index_col=0)\n",
    "\n",
    "restaurants = restaurants[restaurants.business_id.isin(ids)]\n",
    "\n",
    "fastfood = restaurants[restaurants.fastfood==1].business_id\n",
    "bars = restaurants[restaurants.bars==1].business_id\n",
    "japan = restaurants[restaurants.japan==1].business_id\n",
    "asian = restaurants[restaurants.asian==1].business_id\n",
    "\n",
    "feature1 = ['wheelchair','street','cater','waiter','service','waitress','lunch','lot','parking','night','late','reservation']\n",
    "feature2 = ['hipster','late','night','cater','service','waiter','waitress','attire','dressing','kid','dinner','dog']\n",
    "feature3 = ['alcohol','beer','wine','group','party','reservation','street','noise']\n",
    "feature4 = ['noise','intimate','friend','music']\n",
    "\n",
    "\n",
    "reviews = pd.read_csv('bifinal.csv')\n",
    "\n",
    "reviews = reviews[['business_id','stars','reviews']]\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "WC = []\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        index = fastfood\n",
    "        feature = feature1\n",
    "    elif i==1:\n",
    "        index = bars\n",
    "        feature = feature2\n",
    "    elif i == 2:\n",
    "        index = japan\n",
    "        feature = feature3\n",
    "    elif i==3:\n",
    "        index = asian\n",
    "        feature = feature4\n",
    "    tmp = reviews[reviews.business_id.isin(index)]\n",
    "    wc = []\n",
    "    for f in feature:\n",
    "        words = []\n",
    "        for review in tmp.reviews:\n",
    "            line = {}\n",
    "            finds = re.findall('[a-z]* [a-z]* '+f+' [a-z]* [a-z]*',review)\n",
    "            for find in finds:\n",
    "                find = re.sub(f,'',find)\n",
    "                tokens = word_tokenize(find)\n",
    "                for token in tokens:\n",
    "                    if token not in line.keys():\n",
    "                        line[token] = 1\n",
    "                    else:\n",
    "                        line[token] = line[token] + 1\n",
    "            words.append(line)\n",
    "        wc.append(words)\n",
    "    WC.append(wc)\n",
    "\n",
    "\n",
    "restaurants['features'] = ''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "restaurants= restaurants.reset_index(drop=True)\n",
    "\n",
    "restaurants = restaurants.set_index(restaurants.business_id).loc[ids]\n",
    "\n",
    "restaurants = restaurants.reset_index(drop=True)\n",
    "\n",
    "a= 0\n",
    "b=0\n",
    "c=0\n",
    "d=0\n",
    "for x in tqdm(range(restaurants.shape[0])):\n",
    "    \n",
    "    if restaurants.loc[x,'fastfood'] == 1:\n",
    "        restaurants.loc[x,'features'] += 'FASTFOOD: '\n",
    "        for j in range(len(WC[0])):\n",
    "            f = ''\n",
    "            f = '; '.join([y[0]+': '+str(y[1]) for y in WC[0][j][a].items() if len(WC[0][j][a].items()) > 0])\n",
    "            restaurants.loc[x,'features'] += feature1[j].upper() + ': ' + f + ' '\n",
    "        a += 1\n",
    "    if restaurants.loc[x,'bars'] == 1:\n",
    "        restaurants.loc[x,'features'] += 'BARS: '\n",
    "        for j in range(len(WC[1])):\n",
    "            f = ''\n",
    "            f = '; '.join([y[0]+': '+str(y[1]) for y in WC[1][j][b].items() if len(WC[1][j][b].items()) > 0])\n",
    "            restaurants.loc[x,'features'] += feature2[j].upper() + ': ' + f + ' '\n",
    "        b +=1\n",
    "    if restaurants.loc[x,'japan'] == 1:\n",
    "        restaurants.loc[x,'features'] += 'JAPAN: '\n",
    "        for j in range(len(WC[2])):\n",
    "            f = ''\n",
    "            f = '; '.join([y[0]+': '+str(y[1]) for y in WC[2][j][c].items() if len(WC[2][j][c].items()) > 0])\n",
    "            restaurants.loc[x,'features'] += feature3[j].upper() + ': ' + f + ' '\n",
    "        c +=1\n",
    "    if restaurants.loc[x,'asian'] == 1:\n",
    "        restaurants.loc[x,'features'] += 'ASIAN: '\n",
    "        for j in range(len(WC[3])):\n",
    "            f = ''\n",
    "            f = '; '.join([y[0]+': '+str(y[1]) for y in WC[3][j][d].items() if len(WC[3][j][d].items()) > 0])\n",
    "            restaurants.loc[x,'features'] += feature4[j].upper() + ': ' + f + ' '\n",
    "        d +=1\n",
    "\n",
    "final = pd.read_csv('finaldata.csv')\n",
    "\n",
    "final.set_index('business_id').join(restaurants.set_index('business_id')[['fastfood','bars','japan','asian','features']]).to_csv('finaldata2.csv')\n",
    "\n",
    "\n",
    "final2 = pd.read_csv('finaldata2.csv')\n",
    "\n",
    "features = final2.features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for feature in tqdm(features):\n",
    "    tokens = word_tokenize(str(feature))\n",
    "    out = ''\n",
    "    yes = 0\n",
    "    for token in tokens:\n",
    "        if token.isupper():\n",
    "            out = out + token +': '\n",
    "        elif pos_tag([token])[0][1].startswith('J'):\n",
    "            out = out + token + ': '\n",
    "            yes = 1\n",
    "        elif yes == 1 and token != ':':\n",
    "            out = out + token + '; '\n",
    "            yes = 0\n",
    "    new_out=''.join([x for x in re.findall('[A-Z:]+[a-z: 0-9;]+',out) if x.upper()!= x])\n",
    "    new.append(new_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2.features = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final3 = final2.set_index('business_id').join(comments.set_index('ID'),how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(feature):\n",
    "    categories = re.findall('[A-Z]+',str(feature))\n",
    "    features = []\n",
    "    for x in categories:\n",
    "        find = re.findall(x + '[: ;0-9a-z]+',feature)\n",
    "        find = re.sub('([A-Z]+)[:]','\\\\1',find[0])\n",
    "        features.append(find)\n",
    "    return '\\n'.join(features)\n",
    "final3.features = final3.features.apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final3.features.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final3.to_csv('final3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
