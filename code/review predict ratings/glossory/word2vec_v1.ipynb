{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read first 10000 rows from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def read_json_nrows(nrows,filename):\n",
    "    n= 0\n",
    "    with open(filename) as f:\n",
    "        while n < nrows:\n",
    "            if n == 0:\n",
    "                line = f.readline()\n",
    "                line = json.loads(line.rstrip())\n",
    "                train = pd.DataFrame(line,index = [0])\n",
    "            else:\n",
    "                temp = pd.DataFrame(json.loads(f.readline().rstrip()),index = [n])\n",
    "                train = train.append(temp)\n",
    "            n = n+1\n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000 = read_json_nrows(10000,'review_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import numpy as np \n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first tried to apply the *detect* function I got a 'No features in text' error. So I have to find out which review is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000.loc[6687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review is not language, but an emoticon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_language(text):\n",
    "    # First delete all common emoticons.\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)','',text)\n",
    "    if re.sub('[\\W]+','',text) == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, just consider emoticons as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_lang = train_10000[train_10000.text.apply(not_language)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000.loc[not_lang,'lang_type'] = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "languages_ratios = {}\n",
    "for i in range(10000):\n",
    "    tokens = wordpunct_tokenize(train_10000.text[i])\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "    most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
    "    train_10000.loc[i,'lang_type'] = most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "for i in range(10000):\n",
    "    if i in not_lang:\n",
    "        continue\n",
    "    else:\n",
    "        train_10000.loc[i,'lang_type'] = detect(train_10000.text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(':[\\W]{0,1}',':(nmsl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000.lang_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on English only at present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000_eng = train_10000[train_10000.lang_type == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 1)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 2)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 4)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 1)]\n",
    "Counter(text_trigrams).most_common()[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I found two sentences in Chinese:\n",
    "- (('不要在这里吃！',), 1),\n",
    "- (('我们刚在这里吃午饭，在我们的汤里发现了一个蟑螂，我把它展示给服务员，她说：哦，对不起。你不必付饭费\"，这意味着他们的厨房里有很多蟑螂，他们知道，她一点都不惊讶。',),  \n",
    "\n",
    "Most of these rare words have some punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 2)]\n",
    "Counter(text_trigrams).most_common()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emoticons(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    if emoticons == []:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg1 = np.where(train_10000_eng.text.apply(find_emoticons) == True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000_eng.iloc[eg1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can find one  :)  together with two '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('\\\\n','',train_10000_eng.iloc[eg1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most packages can't handle some words like 'nooooo'. \n",
    "def check_same(word):\n",
    "    intervals = {}\n",
    "    for index,letter in enumerate(word):\n",
    "        if letter == word[index-1]:\n",
    "            if letter in intervals.keys():\n",
    "                interval = intervals.pop(letter)\n",
    "                lastguy = interval[len(interval)-1] \n",
    "                if lastguy[1] == (index-1):\n",
    "                    lastguy = (lastguy[0],index)\n",
    "                    interval[len(interval)-1] = lastguy\n",
    "                    intervals[letter] = interval\n",
    "                else:\n",
    "                    lastguy1 = (index-1,index)\n",
    "                    interval.append(lastguy1)\n",
    "                    intervals[letter] = interval\n",
    "            else:\n",
    "                intervals[letter] = [(index-1,index)]\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have never seen a word with three continuous same letter, so I will delete till two.\n",
    "def no_more_than_2(word,dupli):\n",
    "    for key in dupli.keys():\n",
    "        for interval in dupli[key]:\n",
    "            length = interval[1]-interval[0]+1\n",
    "            regex = '(%s'%key + '{%i})'%length\n",
    "            word = re.sub(regex,key+key,word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import suggest\n",
    "from itertools import combinations\n",
    "def right_spelling(word,dupli):\n",
    "    key_list = []\n",
    "    for key in dupli.keys():\n",
    "        key_list.append(key)\n",
    "    n = len(key_list)\n",
    "    for i in range(n):\n",
    "        for comb in combinations(key_list,i):\n",
    "            for letter in comb:\n",
    "                regex = '(%s'%letter + '{2})'\n",
    "                new_word = re.sub(regex,letter,word)\n",
    "                if new_word in brown.words():\n",
    "                    return new_word\n",
    "    return suggest(word)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_typo(word):\n",
    "    if len(word) == len(set(word)):\n",
    "        return word\n",
    "    if suggest(word)[0][1] == 1:\n",
    "        return suggest(word)[0][0]\n",
    "    else:\n",
    "        duplicates = check_same(word)\n",
    "        two = no_more_than_2(word,duplicates)\n",
    "        suggest_two = suggest(two)\n",
    "        if suggest_two[0][1] == 1:\n",
    "            return suggest_two[0][0]\n",
    "        else:\n",
    "            return right_spelling(two,duplicates)\n",
    "        \n",
    "        return right_spelling(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_typo('finaaallly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    tagged = pos_tag(tokens)\n",
    "    for tag in tagged:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think words which mean negative are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.pop(stop.index('but'))\n",
    "stop.pop(stop.index('not'))\n",
    "preposition = ['of','with','at','from','into','during',\n",
    "               'including','until','till','against','among',\n",
    "               'throughout','despite','towards','upon','concerning','to','in',\n",
    "               'for','on','by','about','like','through','over',\n",
    "               'before','between','after','since','without','under',\n",
    "               'within','along','following','across','behind',\n",
    "               'beyond','plus','except','but','up','out','around','down','off','above','near']\n",
    "for prep in preposition:\n",
    "    if prep in stop:\n",
    "        stop.pop(stop.index(prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert n't to not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_abbreviation(text):\n",
    "    text = re.sub('n\\'t',' not',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = ['yet','however','nonetheless','whereas','nevertheless']\n",
    "although = ['although','though','notwithstanding','albeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_but(text):\n",
    "    for x in but:\n",
    "        text = re.sub(x,'but',text)\n",
    "    return text\n",
    "def change_although(text):\n",
    "    for x in although:\n",
    "        text = re.sub(x,'although',text)\n",
    "    return text\n",
    "def change_adversatives(text):\n",
    "    text = change_but(text)\n",
    "    text = change_although(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to capture the key information near but and although."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def although_phrase(text):\n",
    "    words = text.split()\n",
    "    for (index,word) in enumerate(words):\n",
    "        if word == 'altough.':\n",
    "            for x in range(index,index-10,-1):\n",
    "                if re.sub('(.*)\\.([a-z])\\..*','\\\\2',str(wordnet.synsets(words[x])[0])) in ['v','adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # 取表情\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    # 去回车\n",
    "    text = re.sub('\\\\n',' ',text)\n",
    "    # not\n",
    "    text = no_abbreviation(text)\n",
    "    # 只保留字母\n",
    "    text = re.sub('[\\W]+',' ', text.lower())\n",
    "    # 统一转折词\n",
    "    text = change_adversatives(text)\n",
    "    # 词性还原\n",
    "    tokens = lemmatizer(text)\n",
    "    text = ''\n",
    "    for index, token in enumerate(tokens):\n",
    "        # 去拼写错误\n",
    "        #tokens[index] = no_typo(token)\n",
    "        if token in stop:\n",
    "            tokens[index] = ''\n",
    "        else:\n",
    "            text = text + tokens[index] + ' '\n",
    "    return {'text':text,'emoticons':emoticons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "dictionary = train_10000_eng.text.progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_10000_eng.loc[dictionary.index][\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = [dictionary[i]['emoticons'] for i in train_10000_eng.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [dictionary[i]['text'] for i in train_10000_eng.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "new_texts = ['']\n",
    "for i in tqdm(range(len(texts))):\n",
    "    new_texts.append([spell(j) for j in texts[i].split(' ')])\n",
    "\n",
    "new_texts = new_texts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['']\n",
    "for i in range(len(new_texts)):\n",
    "    result.append(' '.join(new_texts[i]))\n",
    "    \n",
    "new_texts = result[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams for phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stream = [sent.split(' ') for sent in new_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(Phrases(sentence_stream, min_count=5, threshold=5)) #mincount越小识别出来的越少，threshold higher means fewer phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts[test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_stream[test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram[sentence_stream[test_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_with_phrase = bigram[sentence_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['']\n",
    "for i in range(len(new_texts)):\n",
    "    result.append(' '.join(bigram[sentence_stream[i]]))\n",
    "    \n",
    "new_texts = result[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type([sentence_with_phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for sent in sentence_with_phrase:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     size=200,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1,\n",
    "                     seed = 123,\n",
    "                     sg=0) # sg默认为0，对应CBOW算法；sg=1则采用skip-gram算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.clock()\n",
    "\n",
    "w2v_model.build_vocab(sentence_with_phrase, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.clock() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.clock()\n",
    "\n",
    "w2v_model.train(sentence_with_phrase, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.clock() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"awesome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"huge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitive Analysis (without tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = np.zeros([len(new_texts),200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(design_matrix.shape[0]):\n",
    "    temp_sent = new_texts[i].split(' ')\n",
    "    for j in range(len(temp_sent)):\n",
    "        design_matrix[i] += w2v_model[temp_sent[j]]\n",
    "    design_matrix[i] = design_matrix[i]/len(temp_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "x = design_matrix[:8000,]\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(x, y[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = y[8000:]\n",
    "y_pred = clf.predict(design_matrix[8000:,])\n",
    "accuracy_score(y_true, y_pred)\n",
    "np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', min_df = 1, lowercase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  tf.fit_transform(new_texts)\n",
    "feature_names = tf.get_feature_names()\n",
    "res_df = pd.DataFrame(response.toarray(),columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = response.toarray()[:8000]\n",
    "tfidf_test = response.toarray()[8000:]\n",
    "y_train = y[:8000]\n",
    "y_test = y[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(multi_class='multinomial',solver='newton-cg’')\n",
    "lr.fit(tfidf_train,y_train)\n",
    "y_pred=lr.predict(tfidf_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(tfidf_train, y_train)\n",
    "y_pred=clf.predict(tfidf_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettoptfidf_dict(df,k): #top k\n",
    "    list1 = []\n",
    "    for i in range(len(df)):\n",
    "        value = sorted(df.iloc[i], reverse = True)[:k] \n",
    "        names = np.array(feature_names)[np.argsort(df.iloc[i])[-k:]][::-1]\n",
    "        dd = dict(zip(names,value))\n",
    "        list1.append(dd)\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_final_dict = gettoptfidf_dict(res_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettoptfidf_word_array(df,k): \n",
    "    list2 = []\n",
    "    for i in range(len(df)):\n",
    "        names = np.array(feature_names)[np.argsort(df.iloc[i])[-k:]][::-1].tolist()\n",
    "        list2.append(names)\n",
    "    return list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_final_word_array = gettoptfidf_word_array(res_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix2 = np.zeros([len(new_texts),200])\n",
    "for i in range(design_matrix2.shape[0]):\n",
    "    temp_sent = tfidf_final_word_array[i]\n",
    "    for j in range(len(temp_sent)):\n",
    "        design_matrix2[i] += w2v_model[temp_sent[j]]\n",
    "    design_matrix2[i] = design_matrix2[i]/len(temp_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "y = train_10000_eng[\"stars\"].values[:9000]\n",
    "x = design_matrix2[:9000,]\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = train_10000_eng[\"stars\"].values[-1000:]\n",
    "y_pred = clf.predict(design_matrix2[-1000:,])\n",
    "accuracy_score(y_true, y_pred)\n",
    "np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
