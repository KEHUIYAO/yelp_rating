{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def read_json_nrows(nrows,filename):\n",
    "    n= 0\n",
    "    with open(filename) as f:\n",
    "        while n < nrows:\n",
    "            if n == 0:\n",
    "                line = f.readline()\n",
    "                line = json.loads(line.rstrip())\n",
    "                train = pd.DataFrame(line,index = [0])\n",
    "            else:\n",
    "                temp = pd.DataFrame(json.loads(f.readline().rstrip()),index = [n])\n",
    "                train = train.append(temp)\n",
    "            n = n+1\n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import os\n",
    "import gc\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('review_train.json',orient = 'records',lines = True)\n",
    "test = pd.read_json('review_test.json',orient = 'records',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:num_samples]\n",
    "train_eng = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # 取表情\n",
    "    # emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    # 去回车\n",
    "    text = re.sub('\\\\n',' ',text)\n",
    "    # not\n",
    "    # text = no_abbreviation(text)\n",
    "    # 只保留字母\n",
    "    text = re.sub('[\\W]+',' ', text.lower())\n",
    "    # 统一转折词\n",
    "    # text = change_adversatives(text)\n",
    "    # 词性还原\n",
    "    # tokens = lemmatizer(text)\n",
    "    # text = ''\n",
    "    # for index, token in enumerate(tokens):\n",
    "        # if token in stop:\n",
    "        #     tokens[index] = ''\n",
    "        # else:\n",
    "        #     text = text + tokens[index] + ' '\n",
    "    # return {'text':text,'emoticons':emoticons}\n",
    "    return {'text':text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "dictionary_train = train_eng.text.progress_apply(preprocessing)\n",
    "dictionary_test = test.text.progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [dictionary_train[i]['text'] for i in train_eng.index]\n",
    "texts_test = [dictionary_test[i]['text'] for i in test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_eng\n",
    "del test\n",
    "del train\n",
    "del dictionary_train\n",
    "del dictionary_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(texts_train)\n",
    "num_test = len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train.extend(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stream = [sent.split(' ') for sent in tqdm(new_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(Phrases(sentence_stream, min_count=5, threshold=5)) #mincount越小识别出来的越少，threshold higher means fewer phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_with_phrase = bigram[sentence_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['']\n",
    "for i in tqdm(range(len(new_texts))):\n",
    "    result.append(' '.join(bigram[sentence_stream[i]]))\n",
    "    \n",
    "new_texts = result[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentence_stream\n",
    "del result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_with_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     size=200,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores,\n",
    "                     seed = 123,\n",
    "                     sg=0) # sg默认为0，对应CBOW算法；sg=1则采用skip-gram算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.clock()\n",
    "\n",
    "w2v_model.build_vocab(sentence_with_phrase, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.clock() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.clock()\n",
    "\n",
    "w2v_model.train(sentence_with_phrase, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.clock() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(columns=[\"t\"],data = new_texts).to_csv('./first_1000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = pd.read_csv('./first_1000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = new_texts[\"t\"].values\n",
    "new_texts[np.argwhere(pd.isnull(new_texts))] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_y = pd.read_csv(\"thirtyM_y.csv\",header=None).values\n",
    "y = original_y.flatten()\n",
    "y = y[:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = new_texts[:num_samples]\n",
    "test_text = new_texts[num_samples:]\n",
    "y = pd.get_dummies(y).values # one_hot\n",
    "print(train_text.shape,test_text.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\n",
    "print(len(X_train_text),y_train.shape)\n",
    "print(len(X_val_text),y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=' '.join(new_texts)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "num_unique_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_len=[]\n",
    "for text in new_texts:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "max_review_len=np.max(r_len)\n",
    "max_review_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = max_review_len\n",
    "batch_size = 1280\n",
    "epochs = 1\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text) #得到句中每个词的索引,由索引组成data\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words) #每行有max_words列，不全的在前段补成0\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm=Sequential()\n",
    "model_lstm.add(Embedding(max_features,100,mask_zero=True)) #emdding层负责W2V\n",
    "#input_dim: smaples = 100000, timesteps = max_words即一句话的词个数，统一了已经, output_dim = 100即词向量的列数\n",
    "model_lstm.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True)) \n",
    "model_lstm.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model_lstm.add(Dense(num_classes,activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm = load_model('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lstm=model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm=model_lstm.predict_classes(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = np.array(range(1,len(y_pred)+1))\n",
    "header = np.array([['Id','Expected']])\n",
    "y_pred = y_pred_lstm.reshape([-1,1])\n",
    "id = id.reshape([-1,1])\n",
    "ans = np.hstack((id, y_pred))\n",
    "ans = np.vstack((header, ans))\n",
    "np.savetxt(\"TueG1_submit3.csv\", ans, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss,accuracy = model.evaluate(x_test,y_test)\n",
    "\n",
    "print('\\ntest loss',loss)\n",
    "print('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = max_review_len\n",
    "batch_size = 1280\n",
    "epochs = 1\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 498426\n",
    "max_words = 1058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn = Sequential()\n",
    "model_lstm_cnn.add(Embedding(max_features,100,input_length=max_words))\n",
    "model_lstm_cnn.add(Dropout(0.2))\n",
    "model_lstm_cnn.add(LSTM(32,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model_lstm_cnn.add(Conv1D(64,kernel_size=3,padding='valid',activation='relu'))\n",
    "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_lstm_cnn.add(Dropout(0.25))\n",
    "model_lstm_cnn.add(Flatten())\n",
    "model_lstm_cnn.add(Dense(64,activation='relu'))\n",
    "model_lstm_cnn.add(Dropout(0.5))\n",
    "model_lstm_cnn.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_lstm_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding: 49842600 = 498426*100，输出1058的句子长度，100维的embedding size, 即1058*100\n",
    "\n",
    "lstm: 17042 = 4*32*((100+32)+1)即4个gate, 32个unit, 上一次的状态 h(t-1)是怎么和下一次的输入 x(t) 结合（concat）起来的, 就是把二者直接拼起来，x是100维的向量，h(t-1)是32维的，那么拼起来就是132位的向量，加上1个bias, 输出1058的句子长度，32个LSTM的unit，每个有一个最后输出,所以是1058*32\n",
    "\n",
    "1Dconv: 6208 = 64*(3*32 + 1), 64个1维卷积核(图片用2维), 每个size是3，每行都有32个参数，1是bias。输出是kernel每走一步与区域乘出来的值relu后都加起来等到一个和，一共走1056次，头尾没法走，所以输出是1056*64，可以做zero-padding去变成1058.\n",
    "\n",
    "maxpooling: 用kernal size一半大小的区域来找每个区域的最大值，从而筛选掉一半变成528*64\n",
    "\n",
    "flatten: 就是变成了528*64 = 33792个输出\n",
    "\n",
    "fully connected层：2162752 = 64*(33792+1),即64个神经元，每个有33792个w和1个bias，relu后输出每个神经元一个值，也就是64个\n",
    "\n",
    "softmax层：5 class label，共有64*(5+1) = 325个参数，所以是输出5个概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm_cnn = load_model('model_lstm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=2, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.save('model_lstm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm_cnn = load_model('model_lstm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.save('model_lstm_cnn_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model_lstm_cnn.predict_classes(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = np.array(range(1,len(y_pred)+1))\n",
    "header = np.array([['Id','Expected']])\n",
    "y_pred = y_pred.reshape([-1,1])\n",
    "id = id.reshape([-1,1])\n",
    "ans = np.hstack((id, y_pred))\n",
    "ans = np.vstack((header, ans))\n",
    "np.savetxt(\"TueG1_submit7.csv\", ans, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn = load_model('model_lstm_cnn_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.72 using LSTM+CNN+categorical_cross_entrophy+3epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-LSTM+Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
